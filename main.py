import tiktoken
import streamlit as st
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# models
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

###### dotenvë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ëŠ” ì‚­ì œí•´ ì£¼ì„¸ìš” ######
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    import warnings
    warnings.warn("dotenv not found. Please make sure to set your environment variables manually.", ImportWarning)
################################################


MODEL_PRICES = {
    "input": {
        "gpt-3.5-turbo": 0.5 / 1_000_000,
        "gpt-4o": 5 / 1_000_000,
        "claude-3-5-sonnet-20240620": 3 / 1_000_000,
        "gemini-1.5-pro-latest": 3.5 / 1_000_000
    },
    "output": {
        "gpt-3.5-turbo": 1.5 / 1_000_000,
        "gpt-4o": 15 / 1_000_000,
        "claude-3-5-sonnet-20240620": 15 / 1_000_000,
        "gemini-1.5-pro-latest": 10.5 / 1_000_000
    }
}


def init_page():
    st.set_page_config(
        page_title="My Great ChatGPT",
        page_icon="ğŸ¤—"
    )
    st.header("My Great ChatGPT ğŸ¤—")
    st.sidebar.title("Options")


def init_messages():
    clear_button = st.sidebar.button("Clear Conversation", key="clear")
	# # clear_buttonì´ í´ë¦­ë˜ê±°ë‚˜ message_historyì— ì´ëŸ­ì´ ì—†ëŠ” ê²½ìš°ì— ì´ˆê¸°í™”
    if clear_button or "message_history" not in st.session_state:
        st.session_state.message_history = [
            ("system", "You are a helpful assistant.")
        ]


def select_model():
    # ìŠ¬ë¼ì´ë”ë¥¼ ì¶”ê°€í•˜ê³  temperatureë¥¼ 0ë¶€í„° 2ê¹Œì§€ ì„ íƒí•  ìˆ˜ ìˆë„ë¡ í•œë‹¤
	# ì´ˆê¹ƒê°’ì€ 0.0, ëˆˆê¸ˆì€ 0.01ë¡œ í•œë‹¤
	
    temperature = st.sidebar.slider(
        "Temperature:", min_value=0.0, max_value=2.0, value=0.0, step=0.01)

    models = ("GPT-3.5", "GPT-4", "Claude 3.5 Sonnet", "Gemini 1.5 Pro")
    model = st.sidebar.radio("Choose a model:", models)
    if model == "GPT-3.5":
        st.session_state.model_name = "gpt-3.5-turbo"
        return ChatOpenAI(
            temperature=temperature,
            model_name=st.session_state.model_name
        )
    elif model == "GPT-4":
        st.session_state.model_name = "gpt-4o"
        return ChatOpenAI(
            temperature=temperature,
            model_name=st.session_state.model_name
        )
    elif model == "Claude 3.5 Sonnet":
        st.session_state.model_name = "claude-3-5-sonnet-20240620"
        return ChatAnthropic(
            temperature=temperature,
            model_name=st.session_state.model_name
        )
    elif model == "Gemini 1.5 Pro":
        st.session_state.model_name = "gemini-1.5-pro-latest"
        return ChatGoogleGenerativeAI(
            temperature=temperature,
            model=st.session_state.model_name
        )


def init_chain():
    st.session_state.llm = select_model()
    prompt = ChatPromptTemplate.from_messages([
        *st.session_state.message_history,
		("user", "{user_input}")  # ì´ ê³³ì— ë‚˜ì¤‘ì— ì‚¬ìš©ìì˜ ì…ë ¥ì´ ë“¤ì–´ê°„ë‹¤
    ])
    output_parser = StrOutputParser()
    return prompt | st.session_state.llm | output_parser


def get_message_counts(text):
    if "gemini" in st.session_state.model_name:
        return st.session_state.llm.get_num_tokens(text)
    else:
        # Claude 3ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ê³µê°œí•˜ì§€ ì•Šì•„ì„œ, tiktokenì„ ì‚¬ìš©í•´ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤
        # ì •í™•í•œ í† í° ìˆ˜ëŠ” ì•„ë‹ˆì§€ë§Œ, ëŒ€ëµì ì¸ í† í° ìˆ˜ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆë‹¤
        if "gpt" in st.session_state.model_name:
            encoding = tiktoken.encoding_for_model(st.session_state.model_name)
        else:
            encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")  # ì„ì‹œë¡œ ì´ìš©
        return len(encoding.encode(text))


def calc_and_display_costs():
    output_count = 0
    input_count = 0
    for role, message in st.session_state.message_history:
		# tiktoken ìœ¼ë¡œ í† í° ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤
        token_count = get_message_counts(message)
        if role == "ai":
            output_count += token_count
        else:
            input_count += token_count

    # ì´ˆê¸°ìƒíƒœì—ì„œ System Messageì—ë§Œ ì´ë ¥ì— ìˆë‹¤ë©´ ì•„ì§ API í˜¸ì¶œì´ ì´ë£¨ì–´ì§€ì§€ ì•Šì€ê²ƒ
    if len(st.session_state.message_history) == 1:
        return

    input_cost = MODEL_PRICES['input'][st.session_state.model_name] * input_count
    output_cost = MODEL_PRICES['output'][st.session_state.model_name] * output_count
    if "gemini" in st.session_state.model_name and (input_count + output_count) > 128000:
        input_cost *= 2
        output_cost *= 2

    cost = output_cost + input_cost

    st.sidebar.markdown("## Costs")
    st.sidebar.markdown(f"**Total cost: ${cost:.5f}**")
    st.sidebar.markdown(f"- Input cost: ${input_cost:.5f}")
    st.sidebar.markdown(f"- Output cost: ${output_cost:.5f}")


def main():
    init_page()
    init_messages()
    chain = init_chain()

	# ì±„íŒ… ì´ë ¥ í‘œì‹œ (2ì¥ê³¼ ë¹„êµí•´ì„œ ì¡°ê¸ˆ ìœ„ì¹˜ê°€ ë°”ë€ ê²ƒì— ì£¼ì˜)
    for role, message in st.session_state.get("message_history", []):
        st.chat_message(role).markdown(message)

    # ì‚¬ìš©ìì˜ ì…ë ¥ ëª¨ë‹ˆí„°ë§
    if user_input := st.chat_input("ê¶ê¸ˆí•œ ê²ƒì„ ì…ë ¥í•´ì£¼ì„¸ìš”"):
        st.chat_message('user').markdown(user_input)

		# LLMì˜ ì‘ë‹µì„ Streaming ì¶œë ¥
        with st.chat_message('ai'):
            response = st.write_stream(chain.stream({"user_input": user_input}))

		# # ì±„íŒ… ì´ë ¥ì— ì¶”ê°€
        st.session_state.message_history.append(("user", user_input))
        st.session_state.message_history.append(("ai", response))

	# ë¹„ìš©ì„ ê³„ì‚°í•´ì„œ ì¶œë ¥
    calc_and_display_costs()


if __name__ == '__main__':
    main()
